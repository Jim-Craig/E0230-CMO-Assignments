# -*- coding: utf-8 -*-
"""CMO Assignment-3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_JiFPxaR_sAaHxHI2jr7c5DG6SfHbt0L
"""



"""# CMO Assignment -3

#### 3. Use the KKT conditions to solve ConvProb, and arrive at an expression for x∗,and show the intermediate steps. Write code to evaluate the expression and report x∗.
"""

import numpy as np
A = np.array([[2,-4,2,-14],[-1,2,-2,11],[-1,2,-1,7]])
b = np.array([10,-6,-5])

AAT = A @ A.T

AAT.shape

EV, EVectors = np.linalg.eig(AAT)

EV

"""AAT has a 0 eigen value, hence invertible. Thus we need to use Psuedo inverse to ccompute the expression$$x = A^T(AA^T)^{-1}b$$"""

pinv_AAT = np.linalg.pinv(AAT)

x = A.T @ pinv_AAT @ b

np.linalg.norm(x)

np.allclose(np.zeros(3),(A @ x - b))

"""So the value of X calculated from the formula actually solves the constraint provided

#### Use the derived projection operator and implement projected gradient descent to solve ConvProb.<br> The equation for the projection operation on a z $\in \rm I\!R^4$ is given by $$P_c(z) = z[I - A^T(AA^T)^{-1}A] -A^T(AA^T)^{-1}b$$
We'd like to implement the gradient descent into using this equation to respect the constratints enforced. This our iteration step would be:<br> for a feasible point $x^k$ , $$x^{k+1} =P_c(x^k + \alpha \nabla f(x^k))$$
"""

constantTerm1 = A.T @ pinv_AAT @ A
constantTerm2 = (A.T @ pinv_AAT @ b).reshape((4,1))
I = np.eye(4,4)
x = x.reshape((4,1))

def projectPoint(x):
  projectedPoint  = (I - constantTerm1) @ x + constantTerm2
  return projectedPoint

def projectedGradientDescent(alpha = 0.1,initialX = np.ones((4,1)), maxIteration = 1000):
  gradFx = initialX
  XUpdated = projectPoint(initialX - alpha * gradFx)
  tolerance = 1e-10
  iteration = 0
  XUpdatedInitial = initialX
  norm = np.linalg.norm(XUpdated - x)
  normDiff = []
  normDiff.append(norm)
  while( norm > tolerance and iteration < maxIteration):
    XUpdatedInitial = XUpdated
    gradFx = XUpdatedInitial
    XUpdated = projectPoint(XUpdatedInitial - alpha * gradFx)
    norm = np.linalg.norm(XUpdated - x)
    normDiff.append(norm)
    # print(f"Iteration {iteration}, Norm of update: {np.linalg.norm(XUpdated - XUpdatedInitial)}")
    iteration += 1
  print(f"Iteration {iteration}, Norm of update: {np.linalg.norm(XUpdated - XUpdatedInitial)}")
  return XUpdated, normDiff

i = 0.01
normDiffList =[]
while(i < 0.1):
  XUpdated, normDiff = projectedGradientDescent(alpha = i)
  normDiffList.append(normDiff)
  i += 0.01

for normDiff in normDiffList:
  plt.plot(normDiff)

